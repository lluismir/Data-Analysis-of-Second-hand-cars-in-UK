---
title: "Deliverable 3"
author: "Lluís Mir Agustí, Daniel Ruiz Jiménez"
date: \today
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
  html_document:
    toc: no
    toc_depth: '4'
  word_document:
    toc: no
    toc_depth: '4'
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
subtitle: 'Numeric and Binary targets Forecasting Models'
classoption: a4paper
editor_options: 
  chunk_output_type: console
---

## Load Required Packages

```{r}
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("effects","FactoMineR","car", "factoextra","RColorBrewer",
"ggplot2","dplyr","ggmap","ggthemes","knitr", "lmtest","effects", "statmod",
"ResourceSelection","chemometrics","missMDA")

package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})

```

# Load data

```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())

setwd("~/Desktop/treball3def")
filepath<-"~/Desktop/treball3def/"
#load(paste0(filepath,"MyOldCars-1000Clean.RData"))
load(paste0(filepath,"Seed5kClean.RData"))

options(contrasts=c("contr.treatment","contr.treatment"))
```

# Linear Models: Using numerical explanatory variables

```{r}
vars_con 
#mil, tax, mpg, years sell
vars_res
#price, audi
ll<-which(df$years_sell==0);ll #we will try to eliminate weird cases
df$years_sell[ll]<-0.5

ll<-which(df$tax==0);ll 
df$tax[ll]<-0.5

ll<-which(df$mileage==0);ll 
df$mileage[ll]<-0.5

ll<-which(df$mpg==0);ll 
df$mpg[ll]<-0.5

ll<-which(df$price==0);ll 
df$price[ll]<-0.5


#1st linear model with my numeric variables:
m1<-lm(price~mileage+tax+mpg+years_sell,data=df) #
#summary(m1) commented bacause of the length

```

The first model shows us a statistical model with all our numerical variables with no transformations. We can see that when we increase 1 unit of price, all numerical variables decrease, because the car is newer, with less years_sell and mileage, but also with less tax and mpg. The model explains a 50.51% of the variability of the sample. We should apply some transformations to our numerical variables in order to increase it. No p-value is greater than 0.05, so we cannot discard any variable, for the moment.


```{r}
vif(m1) #Variance inflation factor: multicorrelation (if greater than 4, !!!)
par(mfrow=c(2,2))
plot(m1,id.n=0)
```

Analyzing the four plots, we can assure that:
The sample overall is not homoscedastic, the values group especially at the right of the graph. We can observe this phenomenon at the 2 left graphs. Also, it's not normal either. Once the theoretical quantity is equal to 2, the sample starts to deviate. At the bottom right graph, we can see that some of the samples are influential observations.


```{r}
# Basic graphs for model validation
par(mfrow=c(1,1))

library(MASS)
# Target variable transformation?
boxcox(price~mileage+tax+mpg+years_sell,data=df)
# Lambda=0 - log transformation is needed
```

The result of the boxcox function shows that the model 1's lambda centers more or less at the 0 value. This means that we will have to proceed with a logarithmic transformation of the target.


```{r}
# New model:
m2<-lm(log(price)~mileage+tax+mpg+years_sell,data=df) 
#summary(m2) commented bacause of the length
```

This new model is different of the previous one because now tax has a p-value greater than 0.05 and a positive correlation with the target, so it's not significant in our sample. However, the low number of numeric variables makes us question if it's worth eliminating it. We will keep it despite the result. The model now represents 62.29% of the total sample, so it's quite an improvement.

```{r}
#vif(m2) Not changed because explanatory variables have not changed

#Analysis of m2 based on its residuals
par(mfrow=c(2,2))
plot(m2,id.n=0)
```

We don't observe a lot of improvements in the four graphs. It's still heteroscedastic, and not normal (with a bit of improvement). More transformations are coming up.


```{r}
# Transformations to my regresors? Are these necessary?
boxTidwell(log(price)~mileage+tax+mpg+years_sell, data=df[!df$mout=="YesMOut",])
#mil 0.5 -> sqrt
#tax <1 -> -1 or -3/2
#mpg <1 -> ^-1, or -3/2 (1/(sqrt(mpg)³))
#years_sell >1 -> ^2

# Power transformations of the predictors in a linear model

par(mfrow=c(1,1))

# Other tools to validate my linear model:
residualPlots(m2,id=list(method=cooks.distance(m2),n=10)) 
marginalModelPlots(m2) #how good the model picks up data from the sample
avPlots(m2,id=list(method=cooks.distance(m2),n=5)) #more slope, more significant is the variable
crPlots(m2,id=list(method=cooks.distance(m2),n=5))
# Objective: Check linearity of my data (model and fit data) and check how my model fits to my data 

```

The boxTidwell function recommends us certain transformations for our regressors. As we can see, we need to do the square root of mileage, keep tax as it is, because the boxTidwell value is very strange, and elevate mpg to the power of -1. The residual plots show us that the sample is not very linear, the blue lines curve a little. The marginal model plots show us that the model fits our data pretty well, except in years_sell. But for now it's ok, we haven't done any transformations yet. Av plots show us how significant are the variables. And Cr plots show us that in general, the model captures the data of the variables moderately.


```{r}
# Model suggested by BoxTidwell
m3<-lm(log(price)~sqrt(mileage)+tax+I(mpg^-1)+years_sell,data=df[!df$mout=="YesMOut",])
#summary(m3) commented bacause of the length

# Validation and effects consideration:
Anova(m3) #Net effect test
par(mfrow=c(2,2))
plot(m3,id.n=0)
par(mfrow=c(1,1))
#residualPlots(m3,id=list(method=cooks.distance(m3),n=10)) 
#marginalModelPlots(m3)
#avPlots(m3,id=list(method=hatvalues(m3),n=5))
#crPlots(m3,id=list(method=cooks.distance(m3),n=5))

```

Moving onto the third model, we have applied the adequate transformation to each one of our regressors. As we can see in the summary and in the Anova test, all variables are now significant to the model, so we don't have to discard any, but the explainability of the model has decreased in comparisson to the previous one. Now it explains 57.05% of the sample. The plots now show an improvement in both homoscedasticity and normality, but it's still not as good as we would want. The residual and marginal plots show that overall the regressors are linear, except tax and years_sell. Cr plots are good except tax, we will do further transformations. In the next model, we will apply transformations to the years_sell variable. 

```{r}
# What about years_sell? 
m4<-lm(log(price)~sqrt(mileage)+tax+I(mpg^-1)+poly(years_sell,2),data=df[!df$mout=="YesMOut",])
#summary(m4)  commented bacause of the length

# Validation and effects consideration:
Anova(m4) #Net effect test
par(mfrow=c(2,2))
plot(m4,id.n=0) #good homoscedasticity

#par(mfrow=c(1,1))
#residualPlots(m4,id=list(method=cooks.distance(m4),n=10))
#marginalModelPlots(m4) 
#avPlots(m4,id=list(method=hatvalues(m4),n=5))
#crPlots(m4,id=list(method=cooks.distance(m4),n=5))
```

In the fourth model, we have a little improvement in explainability (now 57.28%) because we have modified years_sell. In the plots, the homoscedasticity has further improved, but the normality has reached a point where it can't be improved using only numeric variables. The residual plots show overall that the variables are now linear, and the fitted values have a very straight blue line.


```{r}
df2 <- df[!df$mout=="YesMOut",] 

m5<-lm(log(price)~sqrt(mileage)+I(mpg^-1)+poly(years_sell,2),data=df2)
#summary(m5)
#par(mfrow=c(2,2))
#plot(m5,id.n=0)
#par(mfrow=c(1,1))
#residualPlots(m5,id=list(method=cooks.distance(m5),n=10))
#marginalModelPlots(m5) 
#avPlots(m5,id=list(method=hatvalues(m5),n=5))
#crPlots(m5,id=list(method=cooks.distance(m5),n=5))

library(lmtest)
bptest(m5) 
anova(m4, m5)

```

Now onto the fifth model, we have decided to remove the tax variable from the previous one to check if it's significative. The explainability of the model has decreased a little, as well as the normality and homoscedasticity. Now, we have done the Breusch-Pagan test, and the p-value is lower than 0.05, so it tells us that the model isn't still homoscedastic, even though it looks like it. Finally, the anova tells us that there is a difference of significability between the 2 models, so we will keep tax. 


```{r}
m5 <- step( m4, k=log(nrow(df))) #BIC criteria
llres <- which(abs(rstudent(m4))>4);length(llres) #removing residuals
#df[llres,]
par(mfrow=c(1,1))
influencePlot(m5, id=list(n=10)) #influential observations -> hat values
boxplot(cooks.distance(m5))
```

The step function tells us if there is an optimal model starting from m4 with less variables that would make the model better. However, we don't need to discard any regressor. Moving on, we can see the influence plots, to show the influent hat values from our sample. Our goal is to eliminate them a posteriori. 

## Adding factors

We are going to update our model adding some factors, to check if it improves in both normality and homoscedasticity.


```{r}
m5 <- update(m5, ~.+fuelType+transmission+manufacturer+cilindrada+gama,data=df[!df$mout=="YesMOut",])
#summary(m5) commented bacause of the length
m5 <- update(m5, ~.-tax,data=df[!df$mout=="YesMOut",]) #we remove tax
#summary(m5) commented bacause of the length
Anova(m5) 
par(mfrow=c(2,2))
plot(m5,id.n=0)
par(mfrow=c(1,1))
#residualPlots(m5,id=list(method=cooks.distance(m5),n=10))
marginalModelPlots(m5)
#avPlots(m5,id=list(method=hatvalues(m5),n=5))
crPlots(m5,id=list(method=cooks.distance(m5),n=5))
library(effects)
#plot(allEffects(m5))

```

Having added the factors fuelType, transmission, manufacturer, our engineSize factor cilindrada and our price factor gama, we can see that the explainability increases significantly (87.62%). All of the variables that we have added are significant, but tax has a p-value greater than 0.05. We will remove it because we will do interactions where tax is involved (and its factor aux_tax). We have lost .01% of explainability, so it's not a big loss.

```{r}
m6<-update(m5, ~.+fuelType*aux_tax+engineSize+engineSize*mpg,data=df[!df$mout=="YesMOut",]) 
#2 interactions: factor-factor and factor-numeric
#summary(m6)
anova(m5, m6) #p-value < 0.05, m6 is significantly different to the previous model
par(mfrow=c(2,2))
plot(m6,id.n=0)
#plot(allEffects(m6))
m7 <- step( m6, k=log(nrow(df[!df$mout=="YesMOut",])))
m8 <- step( m5, k=log(nrow(df[!df$mout=="YesMOut",])))
AIC(m5,m6,m7,m8) 
```

In our sixth model, we have added 2 interactions: one between 2 factors (fuelType and aux_tax), to check if the taxes of a car have something to do with its fuelType, and one between a factor and a covariate (engineSize and mpg), to check if the mpg of a car depends on the size of its engine. The explainability of the sample has further improved (91.07%), but as we can see, not all of the new variables are significant, and some of the old one have become non singificant too. In order to fix this, we will use the step function to create new models with the optimal variables. Finally, we will execute the AIC function between the 4 last models, and the one with the smallest AIC value will be the one we keep. In this case, m7 has the lowest value.


## Diagnostics for numeric variables:

```{r}
# Define our working dataset 
dfwork <- df[!df$mout=="YesMOut",]

# Imagine that this is our final model: m7
#summary(m7) 
Anova(m7)
#plot(allEffects(m7))
par(mfrow=c(2,2))
plot(m7,id.n=0)

# Define initial parameters:
p <-length(m7$coefficients)
n <- length(m7$fitted.values)
h_param <- 3

# Residual analysis:
llres <- which(abs(rstudent(m7))>3);llres
par(mfrow=c(1,1))
influencePlot(m7, id=list(n=10))
Boxplot(cooks.distance(m7),id=list(labels=row.names(dfwork)))

# A priori influential observation
ll_priori_influential <- which(abs(hatvalues(m7))>h_param*(p/n))
length(ll_priori_influential) #92 influential a priori

# A posteriori influential observation:
ll_posteriori_influential<-which(abs(cooks.distance(m7))>(4/(n-p)));
length(ll_posteriori_influential)
ll_unique_influential<-unique(c(ll_priori_influential,ll_posteriori_influential));
length(ll_unique_influential)
#221 a posteriori influential

m8<-update(m7, data=dfwork[-ll_posteriori_influential,]) 
#new model without these influential values
#summary(m8) commented bacause of the length
Anova(m8) 
plot(allEffects(m8)) #the quality in the pdf of this plot is very vad, so i have commented it
par(mfrow=c(2,2))
plot(m8,id.n=0)
par(mfrow=c(1,1))
marginalModelPlots(m8)
#avPlots(m8)
#crPlots(m8)
influencePlot(m8)
```

In this final part of the deliverable, we will try to eliminate influential observations from our sample. We have checked our model 7 and, as we can see, it's very good in both normality and homoscedasticity. However, we can see that the influential observations are still present. For that reason, we have eliminated them in a new model, model 8. 
The explainability of the sample has raised to 92.29%, and the Anova function tells us that all of our variables are significant. Regarding the all effects plot, we can see how the target variables influences our numeric and categoric regressors. 
The cars with more price are the one that have less mileage, and that have been sold recently. In our sample, the most expensive cars are SemiAuto, followed by Auto and Manual. The most expensive manufacturer in our sample is Mercedes, followed by Audi, BMW and Volkswagen respectively. The gama variable is a bit useless, because it's a categorization of the price, so the more price a car has, the higher its gama is.
Moving on to the interactions, the factor-factor tells us that Diesel cars are more expensive in price but have less taxes, while Petrol cars are cheaper (probably because they are older), but have more taxes (environemental reasons?). The factor-numeric interaction is a bit confusing, but I've come to the conclusion that the engineSize of a car doesn't influentiate on its mpg. We can see that for each category of mpg we have a wide range of engine sizes, that repeat in each mpg.

The plots show us that the sample is now very normally distributed, and homoscedastic. Finally, our influencePlot shows us that there are still 2 influential observations: car 23719 and car 28765.

# Binary Regression

## Split data 80-20

First of all we make a split of the data of 80-20, to carry out the work.
Moreover we have had to make a change in the gama variable, because we had a problem in the predict, and we have solved it with the cut.

```{r}
df$gama <- cut(df$price, breaks=quantile(df$price, seq(0,1,0.25)), labels = 
c("f.Game-baixa", "f.Gama-mitja", "f.Gama-alta", "f.Gama-premium"), 
include.lowest = T)
summary(df$gama)
llwork <- sample(1:nrow(df),round(0.80*nrow(df),0))

dfall<-df
df_train <- dfall[llwork,]
df_test <-dfall[-llwork,]
```

## Selecting the best model

Now we try to get the best model for our data. 
```{r}
library(MASS)
vars_con
ll<-which(df_train$years_sell==0);ll
df$years_sell[ll]<-0.5

ll<-which(df_train$tax==0);ll
df$tax[ll]<-0.5

ll<-which(df_train$mpg==0);ll
df$mpg[ll]<-0.5

ll<-which(df_train$mileage==0);ll
df$mileage[ll]<-0.5

```

In the m1 model we add only the numerical variables, in this case all the variables are significant except years_sell. If we analyze with the vif we will see that there is a bit of collinearity between mileage and years_sell. So we will make another model but without year_sell

```{r}
m1<-glm(Audi~mileage+tax+mpg+years_sell,family="binomial",data=df_train)
#summary(m1) commented because of the length
vif(m1)
Anova(m1,test="LR")

m1_1 <- glm(Audi~mileage+tax+mpg,family="binomial",data=df_train)
#summary(m1_1) commented because of the length
vif(m1_1)
Anova(m1_1,test="LR")

```

We will see that the model m1_1 without year_sell tells us that all the variables are significant and improves the model a bit, it also becomes simpler.

Now we will make a transformation of the model adding square polynomials.

```{r}
m2 <- glm(Audi~ poly(mileage,2)+poly(tax,2)+poly(mpg,2)+poly(years_sell,2),family="binomial",data=df_train)
#summary(m2) commented because of the length
vif(m2)
Anova(m2,test="LR")

m2_1 <- glm(Audi~ poly(mileage,2)+poly(tax,2)+poly(mpg,2),family="binomial",data=df_train)
#summary(m2_1) commented because of the length
vif(m2_1)
Anova(m2_1,test="LR")

AIC(m1,m1_1,m2,m2_1)
anova(m2,m2_1, test = "LR")

```

After testing we realize that we get better results if we have year_sell in our model.


Now we want to add the multivariate outliers, to see how it affects our model.

```{r}
m3<-glm(Audi~mileage+tax+mpg+years_sell,family="binomial",
        data=df_train[!df_train$mout=="YesMOut",])
#summary(m3) commented because of the length
Anova(m3, test="LR")
vif(m3)
```

We can see that m3 there is a great improvement in AIC, and also mileage has lost significance.

```{r}

m3_1<-glm(Audi~1,family="binomial",data=df_train[!df_train$mout=="YesMOut",])
#summary(m3_1) commented because of the length

```

We improve our model with the multivariant outlier, to make it clearer and simpler, with the step function.
We can see that this model is the best and at the same time it is one of the simplest, which we also have to take into account when selecting a model.Where also all the variables of the model are significant.

```{r}
m4 <- step( m3)
Anova(m4, test="LR")
#summary(m4)  commented because of the length
anova( m4, m3, test="Chisq")
waldtest( m4, m3, test="Chisq")
x2 <- cbind(df_train$mpg[!df_train$mout=="YesMOut"])
z<-glm.scoretest(m4,x2);z # library statmod
2*(1-pnorm(abs(z)))
llres <- which(abs(rstudent(m4))>2.5);length(llres)
df_train[llres,]
```

## Adding factors

Now we will add factor variables in our model, for this same reason we will have to put all the numerical variables, because by adding factors some can become significant. And obviously we are going to keep the multivariant outliers

```{r}
m4_1<-glm(Audi~mileage+tax+mpg+years_sell,family="binomial",
          data=df_train[!df_train$mout=="YesMOut",])

m5 <- update(m4_1, ~.+fuelType+transmission+cilindrada+gama,
             data=df_train[!df_train$mout=="YesMOut",])
vif(m5)
#summary(m5)  commented because of the length
Anova(m5, test="LR")
vif(m5)
```

When adding the factor variables, the model is much better compared to the initial model, we also see that all the variables are significant except fuelType.
We will try to improve this model with the step function, in order to make it simpler and a much better model and  we will get the model m6

```{r}
m6<- step(m5)
vif(m6) 
#summary(m6)  commented because of the length
Anova(m6, test="LR")
anova( m6, m5, test="Chisq")

plot(allEffects(m6),selection = 1)
plot(allEffects(m6),selection = 2)
plot(allEffects(m6),selection = 3)
plot(allEffects(m6),selection = 4)
plot(allEffects(m6),selection = 5)
plot(allEffects(m6),selection = 6)
plot(allEffects(m6),selection = 7)
plot(allEffects(m6),selection = 8)
AIC(m1,m1_1,m2,m2_1,m3,m3_1,m4,m5,m6)
#summary(m6)  commented because of the length
```

We see that the output of the step leaves us with the same model, there is no modification with respect to m5.

If we analyze the plots, we can see that Audis cars have a growing relationship with respect to mileage, and that they are cars that have a high tax,the transmission is manual, twice as automatic.In the fuel Type there is more probability that they are petrol but there is not much difference with diesel. Regarding the cilindrada, it is very even in the three categories and if we analyze the gama, they are usually mid-gama or more.

## Interactions

In the first iteraction we have on the m7 model, we apply a step to improve the model and then we see if you have to remove any samples from our model.
In the plot we see the interaction transmission:gama, where we see that there is more probability that the manual and medium-high gama are Audi, although the automatic ones also have a lot of probability with respect to the semi-auto.

```{r}
m7 <- update(m4_1, ~.+(fuelType+transmission+cilindrada+gama)^2,
             data=df_train[!df_train$mout=="YesMOut",])

m8 <- step(m7)
Anova(m8, test="LR")
#summary(m8) commented because of the length
plot(allEffects(m8),selection = 7)
```

In the second interaction we have tried to take two variables that are not very related, the mileage and the transmission.
We can see from the graphs that high mileage and the transmision is manual or semi-automatic, they cars are more likely to be Audi than high mileage automatic cars.

````{r}
residualPlots(m8,id=list(method=cooks.distance(m8),n=10))
marginalModelPlots(m8)
plot(df_train$Audi~df_train$transmission)
plot(df_train$Audi~df_train$mileage)
kruskal.test(df_train$mileage~df_train$transmission)
m9<- update(m8, ~.+mileage:transmission,
            data=df_train[!df_train$mout=="YesMOut",])
Anova(m9)
#summary(m9) commented because of the length

plot(allEffects(m9),selection = 8)

residualPlots(m9,id=list(method=cooks.distance(m9),n=10))
marginalModelPlots(m9)

AIC(m7,m8,m9)
```

## Diagnostic

In the diagnostic of our model we can see that in Normal Q-Q, in the first half some standardized residuals are shown that fit the normal distribution, but the second part rises exponentially, where there are even some outliers.
In scale-location you can also see that the per function follows when it arrives in the middle the value is raised
In the residual vs Leverage you can see a concentration of points in the upper and lower left part of the graph.

```{r}

dfwork <- df_train[!df_train$mout=="YesMOut",]

m10<-glm(Audi ~ mileage + tax + mpg + years_sell + fuelType + 
    transmission + cilindrada + gama + fuelType:cilindrada + 
    transmission:cilindrada + transmission:gama+
      cilindrada:gama+mileage:transmission,family="binomial",data=dfwork)

p <- length(m10$coefficients)
n <- length(m10$fitted.values)
#vif(m10) commented bacause of the length
par(mfrow=c(1,1))
Boxplot(abs(rstudent(m10)))
llres <- which(abs(rstudent(m10))>2.3);llres
Boxplot(hatvalues(m10),id=list(labels=row.names(dfwork)))
llhat <- which(hatvalues(m10)>(3*(p/n)));llhat

influencePlot(m10, id=list(n=10))
Boxplot(cooks.distance(m10),id=list(labels=row.names(dfwork)))
llout<-which(abs(cooks.distance(m10))>0.02);length(llout)
llrem<-unique(c(llout,llres));llrem

m13_1<-glm(Audi ~ mileage + tax + mpg + years_sell + fuelType + 
    transmission + cilindrada + gama + fuelType:cilindrada + 
    transmission:cilindrada + transmission:gama+
    cilindrada:gama+mileage:transmission,family="binomial",data=dfwork[-llrem,])
m0<-glm(Audi ~ 1, family="binomial", data=dfwork[-llrem,])
#summary(m10_1) commented bacause of the length
Anova(m13_1)
plot(allEffects(m10),selection = 5)
plot(allEffects(m10),selection = 6)
marginalModelPlots(m13_1)
#avPlots(m13_1) commented bacause of the length
#crPlots(m13_1) commented bacause of the length
plot(m13_1)
influencePlot(m13_1)
outlierTest(m13_1)

```

## Goodness of fit and Predictive Capacity

To conclude we can see that our model has a prediction of 80 percent to be audi. Furthermore if we look at the ROC curve we see that the diagonal is not very close either, which is good for the model.

```{r}
# H0: Model fits data
Anova(m13_1)
pchisq(m13_1$null.deviance-m13_1$deviance, 
       m13_1$df.null-m13_1$df.residual, lower.tail = FALSE)

X2m13_1<-sum((resid(m13_1,"pearson")^2));X2m13_1
1-pchisq( X2m13_1, m13_1$df.res)

# PseudoR2
library(DescTools)
PseudoR2(m13_1, which='all') # Not working for grouped data
# Sheather
1 - (m13_1$deviance / m10$null.deviance)
# McFadden
1-(as.numeric(logLik(m13_1))/as.numeric(logLik(m0)))
library(ResourceSelection)

pred_test <- predict(m13_1, newdata=df_test, type="response")

ht <- hoslem.test(df_test$Audi, pred_test)
ht
cbind(ht$observed, ht$expected)
# ROC Curve

library("ROCR")
library("AUC")

dadesroc<-prediction(pred_test,df_test$Audi)
par(mfrow=c(1,2))
performance(dadesroc,"auc",fpr.stop=0.05)
plot(performance(dadesroc,"err"))
par(mfrow=c(1,1))
plot(performance(dadesroc,"tpr","fpr"))
abline(0,1,lty=2)
#roc(pred_test,df_test$Audi) commented because of the length

library(cvAUC)
AUC(pred_test,df_test$Audi)

# Confusion Table Analysis

treshold <- 0.5
audi.est <- ifelse(pred_test<treshold,0,1)
tt<-table(audi.est,df_test$Audi);tt
100*sum(diag(tt))/sum(tt)

```
